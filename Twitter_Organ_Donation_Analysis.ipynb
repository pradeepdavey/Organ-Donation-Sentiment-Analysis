{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da676ef",
   "metadata": {},
   "source": [
    "## A Social Network Intervention for Improving Organ Donation Awareness in the US"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869bbb2",
   "metadata": {},
   "source": [
    "### Objective: \n",
    "The study aims to track the digital markers of organ donation in the United States on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a73dce",
   "metadata": {},
   "source": [
    "### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec00f5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from urllib.parse import urlparse\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from city_to_state import city_to_state_dict\n",
    "from two_letter_states import us_state_abbrev\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## Uncomment the lines below to install tweepy, us packages\n",
    "import sys\n",
    "#!{sys.executable} -m pip install tweepy\n",
    "#!{sys.executable} -m pip install us\n",
    "import tweepy\n",
    "import us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656825f",
   "metadata": {},
   "source": [
    "### Access Twitter API in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cc929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tw_oauth(authfile):\n",
    "    with open(authfile, \"r\") as f:\n",
    "        ak = f.readlines()\n",
    "    f.close()\n",
    "    auth1 = tweepy.auth.OAuthHandler(ak[0].replace(\"\\n\", \"\"), ak[1].replace(\"\\n\", \"\"))\n",
    "    auth1.set_access_token(ak[2].replace(\"\\n\", \"\"), ak[3].replace(\"\\n\", \"\"))\n",
    "    return tweepy.API(auth1)\n",
    "\n",
    "# OAuth key file\n",
    "authfile = './auth.k.txt'\n",
    "api = tw_oauth(authfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ff61c",
   "metadata": {},
   "source": [
    "### Twitter Data Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311301d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"organ donation OR Transplant OR organ donor\"\n",
    "date_since = \"2021-08-11\"\n",
    "date_until = \"2021-12-31\"\n",
    "max_tweets = 1500\n",
    "   \n",
    "# Collect tweets\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q = search_words,\n",
    "              #lang=\"en\",\n",
    "              since=date_since,\n",
    "              until=date_until,         \n",
    "              tweet_mode = \"extended\").items(max_tweets)\n",
    "\n",
    "data = [[tweet.author.name, tweet.author.screen_name, tweet.author.location, \n",
    "         int(tweet.author.geo_enabled),        \n",
    "         tweet.place.name if hasattr(tweet.place, 'name') else None,\n",
    "         tweet.place.place_type if hasattr(tweet.place, 'place_type') else None,\n",
    "         tweet.place.full_name if hasattr(tweet.place, 'full_name') else None,\n",
    "         \n",
    "         tweet.place.country_code if hasattr(tweet.place, 'country_code') else None,\n",
    "         \n",
    "         tweet.author.description, \n",
    "         int(tweet.author.protected), int(tweet.author.verified), tweet.author.followers_count,\n",
    "         tweet.author.friends_count, tweet.author.listed_count, tweet.author.favourites_count, \n",
    "         tweet.author.statuses_count, tweet.author.created_at, tweet.created_at, \n",
    "         tweet.retweeted_status.full_text if hasattr(tweet, 'retweeted_status') else tweet.full_text,\n",
    "         tweet.source, \n",
    "         tweet.retweet_count, tweet.favorite_count, tweet.lang,        \n",
    "         tweet.entities['hashtags'][0]['text'] if tweet.entities['hashtags'] else None,\n",
    "         tweet.entities.media if hasattr(tweet.entities, 'media') else None,\n",
    "         tweet.entities.poll if hasattr(tweet.entities, 'poll') else None,\n",
    "         1 if tweet.entities['urls'] else 0,\n",
    "         int(tweet.author.default_profile), int(tweet.author.default_profile_image)       \n",
    "        ]\n",
    "          for tweet in tweets]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=data, \n",
    "                    columns=['User_Name', 'Screen_Name', 'User_Location', 'User_Geo_Enabled', \n",
    "                             'Tweet_Geo_Location', 'Tweet_Geo_LocationType', 'Tweet_Geo_LocationFull', \n",
    "                             'Tweet_Geo_CountryCode',\n",
    "                             'User_Description', \n",
    "                             'User_Protected',\n",
    "                            'User_Verified', 'User_Followers_Count', 'User_Friends_Count', 'User_Listed_Count',\n",
    "                            'User_Favorites_Count', 'User_Status_Count', 'User_Since', 'Tweet_Createdon', 'Tweet_Text', \n",
    "                             'Tweet_Source', 'Retweet_Count', \n",
    "                            'Tweet_Favorites_Count', 'Tweet_Language', 'Tweet_Hashtags', 'Tweet_HasMedia', 'Tweet_HasPoll',\n",
    "                             'User_URL', 'User_Default_Profile', 'User_Default_Image'\n",
    "                            ])\n",
    "\n",
    "#df\n",
    "#df[df['User_Default_Image'].notnull()]\n",
    "#df[df['User_Default_Image']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff230f9",
   "metadata": {},
   "source": [
    "### Cleaning the User Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6bdbd",
   "metadata": {},
   "source": [
    "US States are identified from tweet/author location (which ever is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7525be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_abbr(x):\n",
    "    if re.match('({})'.format(\"|\".join(us_state_abbrev.keys()).lower()), x.lower()):\n",
    "        tokens = [re.match('({})'.format(\"|\".join(us_state_abbrev.keys()).lower()), x.lower()).group(0)]\n",
    "    elif re.match('({})'.format(\"|\".join(city_to_state_dict.keys()).lower()), x.lower()):\n",
    "        k = re.match('({})'.format(\"|\".join(city_to_state_dict.keys()).lower()), x.lower()).group(0)\n",
    "        tokens = [city_to_state_dict.get(k.title(), np.nan)]\n",
    "    else:\n",
    "        tokens = [j for j in re.split(\"\\s|,\", x) if j not in ['in', 'la', 'me', 'oh', 'or']]\n",
    "    for i in tokens:\n",
    "        if re.match('\\w+', str(i)):\n",
    "            if us.states.lookup(str(i)):\n",
    "                return us.states.lookup(str(i)).abbr\n",
    "\n",
    "def Get_US_States(row):\n",
    "    if row['Tweet_Geo_LocationFull']:\n",
    "        value = get_state_abbr(str(row['Tweet_Geo_LocationFull']).upper())\n",
    "    elif row['User_Location']:\n",
    "        value = get_state_abbr(str(row['User_Location']).upper())\n",
    "    else:\n",
    "        value = None\n",
    "    return value            \n",
    "            \n",
    "df['US_State'] = df.apply(Get_US_States, axis=1)\n",
    "df['User_Since_Years'] = pd.to_datetime(\"today\").year - pd.to_datetime(df['User_Since']).dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50236109",
   "metadata": {},
   "source": [
    "### Saving the twitter output every iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf11f7",
   "metadata": {},
   "source": [
    "Since the standard API allows to retrieve tweets up to 7 days ago, we are continuously saving them in the 'Combined_result' dataframe. The duplicate tweets are identified and removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2928dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'./Twitter_Output_Temp.xlsx')\n",
    "df_USA = df[df['US_State'].notnull()]\n",
    "\n",
    "## Let us import the 'Twitter_Output.xlsx' file with all tweets that we saved so far\n",
    "df_original = pd.read_excel(r'./Twitter_Output.xlsx')\n",
    "\n",
    "## Adding newer tweets to the 'Twitter_Output.xlsx'\n",
    "Combined_result = pd.concat([df_USA, df_original])\n",
    "Combined_result = Combined_result[~Combined_result.duplicated(['User_Name', 'Tweet_Createdon', 'Tweet_Text'])]\n",
    "Combined_result = Combined_result.reset_index(drop=True)\n",
    "Combined_result.to_excel(r'./Twitter_Output.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b497765",
   "metadata": {},
   "source": [
    "### On to Machine Learning\n",
    "\n",
    "Now we can use a pre-trained Twitter Account Type Classification model using TFIDF + Logistic Regression Classifier. We are going to create a new column 'Individual'. 0 value indicates that the tweet is from an Organization.\n",
    "\n",
    "**Assumption:** If the tweet description (bio) is empty, then it is assumed to be an individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10341e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "vectorizer = \"vectorizer.pkl\"\n",
    "model = \"Twitter_accounttype_classification_model.pkl\"  \n",
    "\n",
    "loaded_vectorizer = joblib.load(vectorizer)\n",
    "loaded_model = joblib.load(model)\n",
    "\n",
    "Combined_result['User_Description'] = Combined_result['User_Description'].fillna('Individual')\n",
    "X_test = Combined_result['User_Description']\n",
    "X_test_tfidf = loaded_vectorizer.transform(X_test)\n",
    "y_predicted = loaded_model.predict(X_test_tfidf)\n",
    "Combined_result['Individual'] = y_predicted\n",
    "Combined_result.to_excel(r'./Twitter_Output.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f9998",
   "metadata": {},
   "source": [
    "### Tweet Analysis\n",
    "\n",
    "Now that we have identified the Twitter user account type, let us look at some trends across US"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a07f8",
   "metadata": {},
   "source": [
    "#### Organ Donation Tweets by US States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a3657",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cr_counts = Combined_result.groupby(['US_State']).size().reset_index(name='# Organ Donation Related Tweets')\n",
    "cr_counts = cr_counts.sort_values('# Organ Donation Related Tweets', ascending=False)\n",
    "\n",
    "####### Map ########\n",
    "\n",
    "#!{sys.executable} -m pip install plotly\n",
    "import plotly.express as px\n",
    "fig = px.choropleth(cr_counts,\n",
    "                    locations='US_State',\n",
    "                    color='# Organ Donation Related Tweets',\n",
    "                    color_continuous_scale='blues',\n",
    "                    hover_name='US_State',\n",
    "                    locationmode='USA-states',\n",
    "                    scope='usa')\n",
    "\n",
    "# Add abbrievated State Labels\n",
    "fig.add_scattergeo(\n",
    "    locations=cr_counts['US_State'],\n",
    "    locationmode='USA-states',\n",
    "    text=cr_counts['US_State'],\n",
    "    mode='text')\n",
    "\n",
    "# Add Map title\n",
    "fig.update_layout(\n",
    "    \n",
    "    title={'text':'Organ Donation Related Tweets by US State Since Aug 11, 2021',\n",
    "           'xanchor':'center',\n",
    "           'yanchor':'top',\n",
    "           'x':0.5}\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "####### Bar chart ########\n",
    "\n",
    "# function to add value labels\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y.iloc[i] + 1.5, y.iloc[i], ha = 'center')\n",
    "        \n",
    "x = cr_counts[0:9]['US_State']\n",
    "y = cr_counts[0:9]['# Organ Donation Related Tweets']\n",
    "fig, ax = plt.subplots(figsize =(12, 6))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(x, y, \n",
    "        #color ='blue',\n",
    "        width = 0.4)\n",
    " \n",
    "plt.xlabel(\"US States (Top 10)\")\n",
    "plt.ylabel(\"# Organ Donation Related Tweets\")\n",
    "plt.title(\"# of Organ Donation Related Tweets by US State (Top 10)\")\n",
    "\n",
    "# Add annotation to bars\n",
    "addlabels(x, y)\n",
    "\n",
    "# Add Text watermark\n",
    "fig.text(0.85, 0.8, 'Since Aug 11, 2021', fontsize = 12,\n",
    "         color ='black', ha ='right', va ='bottom',\n",
    "         alpha = 1)\n",
    "_ = plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd40e7",
   "metadata": {},
   "source": [
    "#### Organ Donation Tweets by US States and User Account Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc028d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x['Type_Individual'] = x['Individual']\n",
    "    x['Type_Organization'] = 1-x['Individual']\n",
    "    return x\n",
    "\n",
    "cr_counts = Combined_result.apply(f, axis=1)\n",
    "cr_counts = cr_counts.groupby('US_State').agg({'Type_Individual': 'sum', 'Type_Organization': 'sum', 'User_Name': 'count'})\n",
    "cr_counts = cr_counts.reset_index()\n",
    "cr_counts = cr_counts.sort_values('User_Name', ascending=False)\n",
    "\n",
    "x = cr_counts[0:9]['US_State']\n",
    "y = cr_counts[0:9]['Type_Individual']\n",
    "z = cr_counts[0:9]['Type_Organization']\n",
    "\n",
    "X_axis = np.arange(len(x))\n",
    "\n",
    "##### # Chart\n",
    "fig, ax1 = plt.subplots(figsize=(8,6))  \n",
    "ax1.bar(X_axis - 0.2, y, 0.4, label = 'Individual')\n",
    "ax1.bar(X_axis + 0.2, z, 0.4, label = 'Organization')\n",
    "  \n",
    "plt.xticks(X_axis, x)\n",
    "plt.xlabel(\"US States\")\n",
    "plt.ylabel(\"# of Tweets\")\n",
    "plt.title(\"Organ Donation Related Tweets by Account Type\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##### % Chart\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "def f(x):\n",
    "    x['% Tweets by Organization'] = x['Type_Organization'] / x['User_Name']\n",
    "    return x\n",
    "\n",
    "cr_counts = cr_counts.apply(f, axis=1)\n",
    "cr_counts = cr_counts.reset_index()\n",
    "cr_counts = cr_counts.sort_values(['User_Name', '% Tweets by Organization'], ascending=False)\n",
    "\n",
    "\n",
    "x = cr_counts[0:9]['US_State']\n",
    "y = cr_counts[0:9]['% Tweets by Organization']\n",
    "fig, ax1 = plt.subplots(figsize=(8,6))  \n",
    "ax1.bar(X_axis, y, 0.5, \n",
    "        label = '% Tweets by Organization',\n",
    "        color = 'darkorange')\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "plt.xticks(X_axis, x)\n",
    "plt.xlabel(\"US States\")\n",
    "plt.ylabel(\"% of Tweets by Organization\")\n",
    "plt.title(\"Organ Donation Related Tweets by Organization\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b602f3",
   "metadata": {},
   "source": [
    "### Twitter Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install textblob\n",
    "#!{sys.executable} -m pip install nltk\n",
    "#!{sys.executable} -m pip install wordcloud\n",
    "#!{sys.executable} -m pip install langdetect\n",
    "#nltk.download('vader_lexicon')\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b96ee4",
   "metadata": {},
   "source": [
    "Let us clean the twitter text first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a60d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\", regex=True)\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "Twitter_Final = Combined_result[~Combined_result['Tweet_Text'].isnull()]\n",
    "Twitter_Final['Tweet_Text_Clean'] = Twitter_Final['Tweet_Text']\n",
    "Twitter_Final = standardize_text(Twitter_Final, 'Tweet_Text_Clean')\n",
    "Twitter_Final[['Tweet_Text', 'Tweet_Text_Clean']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c45a9e",
   "metadata": {},
   "source": [
    "Let us now use Textblob to calculate positive, negative, neutral, polarity and compound parameters from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Twitter_Final[['Polarity', 'Subjectivity']] = Twitter_Final['Tweet_Text_Clean'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in Twitter_Final['Tweet_Text_Clean'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        Twitter_Final.loc[index, 'Sentiment'] = 'Negative'\n",
    "    elif pos > neg:\n",
    "        Twitter_Final.loc[index, 'Sentiment'] = 'Positive'\n",
    "    else:\n",
    "        Twitter_Final.loc[index, 'Sentiment'] = 'Neutral'\n",
    "        Twitter_Final.loc[index, 'Negative'] = neg\n",
    "        Twitter_Final.loc[index, 'Neutral'] = neu\n",
    "        Twitter_Final.loc[index, 'Positive'] = pos\n",
    "        Twitter_Final.loc[index, 'Compound'] = comp\n",
    "Twitter_Final.to_excel(r'./Twitter_Output.xlsx', index = False)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707dbb0c",
   "metadata": {},
   "source": [
    "#### Stop Words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5576be2",
   "metadata": {},
   "source": [
    "Let us first look at the stop words of the english language from the nltk library. We can create our custom stop word list specific to our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f62f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b974d4b",
   "metadata": {},
   "source": [
    "Retaining some words that would be useful during the user account classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fee770",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "                 'herself', 'us']  \n",
    "stopwords = [ele for ele in stopwords if ele not in not_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe1439",
   "metadata": {},
   "source": [
    "Defining the function to remove stopwords from the description tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output= [i for i in text if i not in stopwords]\n",
    "    return output\n",
    "\n",
    "#applying the function\n",
    "df_user_labelled['User_Description_Tokens'] = df_user_labelled['User_Description_Tokens'].apply(lambda x:remove_stopwords(x))\n",
    "\n",
    "df_user_labelled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
